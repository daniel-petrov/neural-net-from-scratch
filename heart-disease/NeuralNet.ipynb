{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "Following [this](https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import everything\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data download from UCL\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# add header names\n",
    "headers = [\n",
    "    'age','sex','chest_pain','resting_blood_pressure','serum_cholestoral',\n",
    "    'fasting_blood_sugar','resting_ecg_results','max_heart_rate_achieved',\n",
    "    'exercise_induced_angina','oldpeak','slope of the peak',\n",
    "    'num_of_major_vessels','thal','heart_disease'\n",
    "]\n",
    "\n",
    "heart_df = pd.read_csv('heart.dat', sep=' ', names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>chest_pain</th>\n",
       "      <th>resting_blood_pressure</th>\n",
       "      <th>serum_cholestoral</th>\n",
       "      <th>fasting_blood_sugar</th>\n",
       "      <th>resting_ecg_results</th>\n",
       "      <th>max_heart_rate_achieved</th>\n",
       "      <th>exercise_induced_angina</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope of the peak</th>\n",
       "      <th>num_of_major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>57.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>270 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  chest_pain  resting_blood_pressure  serum_cholestoral  \\\n",
       "0    70.0  1.0         4.0                   130.0              322.0   \n",
       "1    67.0  0.0         3.0                   115.0              564.0   \n",
       "2    57.0  1.0         2.0                   124.0              261.0   \n",
       "3    64.0  1.0         4.0                   128.0              263.0   \n",
       "4    74.0  0.0         2.0                   120.0              269.0   \n",
       "..    ...  ...         ...                     ...                ...   \n",
       "265  52.0  1.0         3.0                   172.0              199.0   \n",
       "266  44.0  1.0         2.0                   120.0              263.0   \n",
       "267  56.0  0.0         2.0                   140.0              294.0   \n",
       "268  57.0  1.0         4.0                   140.0              192.0   \n",
       "269  67.0  1.0         4.0                   160.0              286.0   \n",
       "\n",
       "     fasting_blood_sugar  resting_ecg_results  max_heart_rate_achieved  \\\n",
       "0                    0.0                  2.0                    109.0   \n",
       "1                    0.0                  2.0                    160.0   \n",
       "2                    0.0                  0.0                    141.0   \n",
       "3                    0.0                  0.0                    105.0   \n",
       "4                    0.0                  2.0                    121.0   \n",
       "..                   ...                  ...                      ...   \n",
       "265                  1.0                  0.0                    162.0   \n",
       "266                  0.0                  0.0                    173.0   \n",
       "267                  0.0                  2.0                    153.0   \n",
       "268                  0.0                  0.0                    148.0   \n",
       "269                  0.0                  2.0                    108.0   \n",
       "\n",
       "     exercise_induced_angina  oldpeak  slope of the peak  \\\n",
       "0                        0.0      2.4                2.0   \n",
       "1                        0.0      1.6                2.0   \n",
       "2                        0.0      0.3                1.0   \n",
       "3                        1.0      0.2                2.0   \n",
       "4                        1.0      0.2                1.0   \n",
       "..                       ...      ...                ...   \n",
       "265                      0.0      0.5                1.0   \n",
       "266                      0.0      0.0                1.0   \n",
       "267                      0.0      1.3                2.0   \n",
       "268                      0.0      0.4                2.0   \n",
       "269                      1.0      1.5                2.0   \n",
       "\n",
       "     num_of_major_vessels  thal  heart_disease  \n",
       "0                     3.0   3.0              2  \n",
       "1                     0.0   7.0              1  \n",
       "2                     0.0   7.0              2  \n",
       "3                     1.0   7.0              1  \n",
       "4                     1.0   3.0              1  \n",
       "..                    ...   ...            ...  \n",
       "265                   0.0   7.0              1  \n",
       "266                   0.0   7.0              1  \n",
       "267                   0.0   3.0              1  \n",
       "268                   0.0   6.0              1  \n",
       "269                   3.0   3.0              2  \n",
       "\n",
       "[270 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(270, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values and check data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        0\n",
       "sex                        0\n",
       "chest_pain                 0\n",
       "resting_blood_pressure     0\n",
       "serum_cholestoral          0\n",
       "fasting_blood_sugar        0\n",
       "resting_ecg_results        0\n",
       "max_heart_rate_achieved    0\n",
       "exercise_induced_angina    0\n",
       "oldpeak                    0\n",
       "slope of the peak          0\n",
       "num_of_major_vessels       0\n",
       "thal                       0\n",
       "heart_disease              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# neural net needs all features to be numeric\n",
    "heart_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                        float64\n",
       "sex                        float64\n",
       "chest_pain                 float64\n",
       "resting_blood_pressure     float64\n",
       "serum_cholestoral          float64\n",
       "fasting_blood_sugar        float64\n",
       "resting_ecg_results        float64\n",
       "max_heart_rate_achieved    float64\n",
       "exercise_induced_angina    float64\n",
       "oldpeak                    float64\n",
       "slope of the peak          float64\n",
       "num_of_major_vessels       float64\n",
       "thal                       float64\n",
       "heart_disease                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train and test sets, then standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train set is (216, 13)\n",
      "Shape of test set is (54, 13)\n",
      "Shape of train label is (216, 1)\n",
      "Shape of test labels is (54, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert input to numpy arrays\n",
    "X = heart_df.drop(columns=['heart_disease']) # predicting heart disease\n",
    "\n",
    "# replace target class with 0 and 1\n",
    "# 1 = 'have heart disease', 0 = 'no heart disease'\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(1, 0)\n",
    "heart_df['heart_disease'] = heart_df['heart_disease'].replace(2, 1)\n",
    "\n",
    "y_label = heart_df['heart_disease'].values.reshape(X.shape[0], 1)\n",
    "\n",
    "# split data into tran and test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y_label, test_size=0.2, \n",
    "                                               random_state=2)\n",
    "\n",
    "# standardise dataset\n",
    "sc = StandardScaler()\n",
    "sc.fit(Xtrain)\n",
    "Xtrain = sc.transform(Xtrain)\n",
    "Xtest = sc.transform(Xtest)\n",
    "\n",
    "print(f'Shape of train set is {Xtrain.shape}')\n",
    "print(f'Shape of test set is {Xtest.shape}')\n",
    "print(f'Shape of train label is {ytrain.shape}')\n",
    "print(f'Shape of test labels is {ytest.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the network\n",
    "The neural network is comprised of multiple layers. Each layer is a collection of nodes. Each node acts as a neuron and performs calculations on the data passed to it. Here's an exmaple of a 3-layer neural network below:\n",
    "\n",
    "![Img Alt Text](https://miro.medium.com/max/700/1*Z3zHoX1nhK6Rsmd4yNPdsg.jpeg)\n",
    "\n",
    "Although there are technically 4 layers, the first layer is not counted. The first layer is the **input layer**, and the number of nodes in this layer will depend on the number of features present in your dataset. In our case it will be 13 nodes (because we have 13 features).\n",
    "\n",
    "The final layer of the neural network is called the **output layer**, and the number of nodes depends on what you're trying to predict. For regression and binary classification tasks, a single node will do; while for multi-class problems, multiple nodes are needed (depending on the number of classes).\n",
    "\n",
    "Our task is **binary classification** (i.e. the patient either has a heart disease or not).\n",
    "\n",
    "The layers between the input and final layers are called the **hidden layers**. The hidden layers can be as deep and/or wide as you want, but although a deeper network is better, it becomes more computationally expensive.\n",
    "\n",
    "To keep things simple in this tutorial, we'll have 1 hidden layer with 8 nodes to create a 2-layer network:\n",
    "\n",
    "![Img Alt Text](https://miro.medium.com/max/586/1*tDZCZM0g5oJxyZ0JfqRJDQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "The weights and biases are the learnable/adjustable parameters which optimise the neural network. Think of it like this:  \n",
    "- Weights: how sure you are that a feature contributes to a prediction.  \n",
    "- Biases: base value that your predictions must start from.\n",
    "\n",
    "Let's go through an example:  \n",
    "Assume you're a machine learning model, and the goal is to predict whether a person is rich or not, and you've been the following clues (*features*) to help with the decision:\n",
    "- Age\n",
    "- Height\n",
    "- Salary\n",
    "\n",
    "The clues above are called the **features** and what you want to predict is the **target** (a.k.a. label/ground truth). In this example the label can be one of two classes (rich, not rich) - in other words, ***binary classification***. Basically, you want to combine the features in such an equation that they help with accurately predicting the outcome.\n",
    "\n",
    "<br>\n",
    "<center>$y(rich, not rich) = Age + Height + Salary + [base]$</center>\n",
    "\n",
    "<br>\n",
    "Let's assume a base salary of \\$3000, and Person 1 has the following features; age = 18, height = 5.6ft, salary = \\$2000, then you'll calculate richness as follows:\n",
    "<br>\n",
    "<br>\n",
    "<center>$y(rich, not rich) = 18 + 5.6 + 2000 + 3000 =  \\sim5024$</center>\n",
    "\n",
    "<br>\n",
    "For this example, we might define a threshold for richness as any value greater than \\$40,000. Judging by these criteria, we might conclude that Person 1 is not rich. Let's look at another example:\n",
    "\n",
    "Person 2 has the following features; age = 26, height = 5.2ft, salary = \\$50,000. The prediction will be calculated as:\n",
    "\n",
    "<br>\n",
    "<center>$y(rich, not rich) = 26 + 5.2 + 50000 + 3000 = \\sim53,031$</center>\n",
    "\n",
    "<br>\n",
    "Then by the threshold stated earlier, Person 2 is rich.\n",
    "\n",
    "Obvioucly some clues are more important than others (Salary), and others are less important (Age and Height). With this idea, we can assign importance to features. For example, we can assign the following *weights*:\n",
    "\n",
    "<center>$y(rich, not rich) = (2\\times Age) + (1\\times Height) + (8\\times Salary) + base$</center>\n",
    "\n",
    "<br>\n",
    "Intuitively, we assign a higher value to the salary feature. The importance of the value can be any number but must be representative of scale. The base value of 3000 is called the **bias** - it's a base value that every prediction must have, even when nothing else is given.\n",
    "\n",
    "Now if you make a prediction for Person 1 and 2 again, you'll have the following:\n",
    "\n",
    "<br>\n",
    "Person 1: $(2\\times 18)+(1\\times 5.6)+(8\\times 2000)+3000=\\sim 19,041$ (still poor)\n",
    "<br>\n",
    "<br>\n",
    "Person 2: $(2\\times 26)+(1\\times 5.2)+(8\\times 5000)+3000=\\sim 43,057$ (still rich)\n",
    "\n",
    "If a person has no value for age, height and salary then the prediction will be the base value. This is need for bias. What you should take away from this is:\n",
    "- Weights: values assigned to features\n",
    "- Biases: Base value\n",
    "\n",
    "A machine learning model uses lots of data to learn the correct weights and biases to assign each feature in a dataset to correctly predict outputs.\n",
    "\n",
    "Back to the tutorial example, you now know that every feature in the dataset must be assigned a weight and that after doing a weighted sum, a bias term is added. This is done in the class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    '''\n",
    "    A two layer neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layers=[13,8,1], learning_rate=0.001, \n",
    "                 iterations=100):\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialise the weights from a random normal distribution\n",
    "        '''\n",
    "        np.random.seed(1) # seed the random number generator\n",
    "        self.params['W1'] = np.random.randn(self.layers[0], self.layers[1])\n",
    "        self.params['b1'] = np.random.randn(self.layers[1],)\n",
    "        self.params['W2'] = np.random.randn(self.layers[1], self.layers[2])\n",
    "        self.params['b2'] = np.random.randn(self.layers[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you create a nerual network class, and during initialisation you create some variables to hold intermediate calculations. You'll notice `layers` matches the dimensions of the neural network we will be building. We'll talk about `learning_rate`, `iterations` and `sample_size` later.\n",
    "\n",
    "Next, you created a function `init_weights()` to initialise the weights and biases as random numbers. These weights are initialised from a random normal distribution and saved to a dictionary called **`params`**.\n",
    "\n",
    "You'll notice there are two weight and bias arrays. The first weight array `W1` will have dimensions of 13 by 8 - this is because you have 13 input features and 8 hidden nodes, while the first bias `b1` will be a vector of size 8 because you have 8 hidden nodes.\n",
    "\n",
    "The second weight array `W2` will be a 8 by 1-dimensional array because you have 8 hidden nodes and 1 output node, and finally the second bias `b2` will be a vector of size 1 because you have just 1 output.\n",
    "\n",
    "Here's an image to illustrate what's happening at each node:\n",
    "\n",
    "![Img Alt Text](https://gblobscdn.gitbook.com/assets%2F-LvBP1svpACTB1R1x_U4%2F-LvI8vNq_N7u3RWVAPLk%2F-LvJSdcFXzoI-WW0L3w5%2Fimage.png?alt=media&token=84526dc6-4634-4de5-aacf-00a179afac76)\n",
    "\n",
    "In general for an n-layer architecture, you will have n arrays of weights and n biases at each layer (excluding the input layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Function\n",
    "Now that weights and biases have been initialised, let's talk about activation functions.\n",
    "\n",
    "***Activations*** *are the nonlinear computations done in each node of a Neural Network.*\n",
    "\n",
    "Remember when I said each node performs a mathematical computation? Well, that happens in two phases:\n",
    "\n",
    "First, you do a weighted sum of the input and the weights, add the biases, and then pass the result through an activation function. Why? Keep reading...\n",
    "\n",
    "An activation function is what makes a neural network capable of learning complex non-linear functions. Non-linear functions are difficult for traditional machine learning algorithms (like logistic and linear regression) to learn. The activation function is what makes a neural network capable of understanding these functions.\n",
    "\n",
    "There are many types of activation functions used in deep learning. Each function has its pros and cons, but the ReLU function has been shown to perform very well, so let's use that.\n",
    "\n",
    "![Img Alt Text](https://miro.medium.com/max/700/1*ZafDv3VUm60Eh10OeJu1vw.png)\n",
    "\n",
    "The activation function is computed by each node in the hidden layers of a neural network. This means you'll have to pass the weighted sums through the ReLU function for every node in the hidden layers.\n",
    "\n",
    "But what is ReLU?\n",
    "\n",
    "ReLU (Rectified Linear Unit) is a simple function that compares a value with zero. That is, it will return the value passed to it if it is greater than zero; otherwise it returns zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to be added to Neural Network class\n",
    "class NeuralNet1(NeuralNet):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet.__init__(self, *args)\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        '''\n",
    "        The ReLU activation function is to perform a threshold\n",
    "        operation to each input element where negative values\n",
    "        are set to zero.'''\n",
    "        return np.maximum(0, Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `relu()` function is to be added to the `NeuralNet` class (here I extended the original class to add the method - the full class will be at the very bottom). This function performs an array-wise ReLU because you'll be dealing mainly with arrays, not single values.\n",
    "\n",
    "In summary, the hidden layers receives values from the input layer, calculates a weighted sum, adds the bias term, and then passes each result through an activation function - in our case a ReLU. The result from the ReLU is then passed to the output layer, where another weighted sum is performed using the second weights and biases. but then instead of passing the result through another activation function, it is passed through an output function.\n",
    "\n",
    "The output function will depend on what you're trying to predict. You can use a sigmoid function when you have a two-class problem (binary classification), and you can use a function called softmax for multi-class problems.\n",
    "\n",
    "In this tutorial, we'll be using the sigmoid function for the output layer. This is because you're predicting one of two classes.\n",
    "\n",
    "![Img Alt Text](https://miro.medium.com/max/640/1*OUOB_YF41M-O4GgZH_F2rw.png)\n",
    "\n",
    "The sigmoid function takes a real number and squashes it to a value between 0 and 1. In other words, it outputs a probability score for every real number. This is useful for the task at hand because you don't just want your model to predict a yes (1) or no (0) - you want it to predict probabilities that can help you measure how sure it is of its predictions.\n",
    "\n",
    "Let's add the sigmoid function to our `NeuralNet` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(NeuralNet1):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet1.__init__(self, *args)\n",
    "        \n",
    "    def sigmoid(self, Z):\n",
    "        '''\n",
    "        The sigmoid function takes in real numbers in any range and\n",
    "        squashes it to a real-valued output between 0 and 1.\n",
    "        '''\n",
    "        return 1/(1+np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy exponential function is used as it is now possible to perform the operation for arrays instead of single values. Also, Numpy implementation is faster than pure Python, as it's written in C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "The loss function is a way of measuring how good a model's prediction is so that it can adjust the weights and biases.\n",
    "\n",
    "A loss function must be properly designed so that it can correctly penalise a model that is wrong and reward a model that is right. This means that you want the loss to tell you if a prediction made is far or close to the true prediction.\n",
    "\n",
    "The choice of the loss function is dependent on the task - and for classification problems, you can use cross-entropy loss.\n",
    "<br>\n",
    "<br>\n",
    "<font size=\"5\"><center>$CE=-\\sum_{i}^C y_i\\log{(\\hat{y}_i)}$</center></font>\n",
    "\n",
    "Where:\n",
    "- $C$ = number of classes\n",
    "- $y_{i}$ = true value\n",
    "- $\\hat{y}_{i}$ = predicted value  \n",
    "\n",
    "For a binary classification task (i.e. $C=2$), the cross-entropy loss function becomes:\n",
    "\n",
    "<font size=\"4\"><center>$CE=-\\sum_{i=1}^2 y_1\\log{(\\hat{y})}=-y_1\\log{(\\hat{y}_1)}-(1-y_1)\\log{(1-\\hat{y}_1)}$</center></font>\n",
    "\n",
    "Let's put this into code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if NN supplies 0 values to log, it will result in infinity, which will \n",
    "# affect network training. So here, compare value and if it is zero, \n",
    "# replace it with an extremely small value\n",
    "class NeuralNet3(NeuralNet2):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet2.__init__(self, *args)\n",
    "        \n",
    "    def eta(self, x):\n",
    "        ETA = 0.0000000001\n",
    "        return np.maximum(x, ETA)\n",
    "    \n",
    "    def entropy_loss(self, y, y_hat):\n",
    "        nsample = len(y)\n",
    "        y_hat_inv = 1.0 - y_hat\n",
    "        y_inv = 1.0 - y\n",
    "        y_hat = self.eta(y_hat_inv) # clips value to avoid NaNs in log\n",
    "        y_hat_inv = self.eta(y_hat_inv)\n",
    "        \n",
    "        loss = -1/nsample * (\n",
    "            np.sum(np.multiply(np.log(y_hat), y) + \n",
    "                   np.multiply((y_inv), np.log(y_hat_inv))\n",
    "                  )\n",
    "        )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the sum and division by sample size? This means you're considering the average loss with respect to all the inputs. That is, you're concerned about the combined loss from all the samples and not the individual losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going Forward: Forward Propagation\n",
    "Forward propagation is the name given to the series of computations performed by the neural network before a prediction is made. In our two-layer network, we'll perform the following computation for forward propagation:\n",
    "- Compute the weighted sum between the input and first layer's weights and then add the bias: $Z_1=(W_1\\times X)+b$\n",
    "- Pass the result through the ReLU activation function: $A_1=relu(Z_1)$\n",
    "- Compute the weighted sum between the output ($A_1$) of the previous step and the second layer's weights - also add the bias: $Z_2=(W_2\\times A_1)+b_2$\n",
    "- Compute the output function by passing the result through a sigmoid function: $A_2=\\sigma(Z_2)$\n",
    "- And finally, compute the loss between the predicted output and the true labels: $loss(A_2, Y)$\n",
    "\n",
    "![Img Alt Text](neural-net-diagram.png)\n",
    "\n",
    "Where:  \n",
    "$a_0=relu((x_0\\times w_{0_{1,1}})+b_{0_1})$  \n",
    "$y0=\\sigma((a0\\times w_{1_1})+b)$\n",
    "\n",
    "And there, you have the forward propagation for your two-layer neural network. For a three-layer network, you'd have to compute the $Z_3$ and $A_2$ using $W_3$ and $b_3$ before the output layer.\n",
    "\n",
    "Now let's add the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet4(NeuralNet3):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet3.__init__(self, *args)\n",
    "        \n",
    "    def forward_propagation(self):\n",
    "        '''\n",
    "        Performs the forward propagation\n",
    "        '''\n",
    "        \n",
    "        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        y_hat = self.sigmoid(Z2)\n",
    "        loss = self.entropy_loss(self.y, y_hat)\n",
    "        \n",
    "        # save calculated parameters\n",
    "        self.params['Z1'] = Z1\n",
    "        self.params['Z2'] = Z2\n",
    "        self.params['A1'] = A1\n",
    "        \n",
    "        return y_hat, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, first you perform all the dot products and additions using the weights and biases you initialised earlier, calculate the loss by calling the `entropy_loss()` method, save the calculated parameters and finally return the predicted values and loss. These values will be used during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Step Backward: Backpropagation\n",
    "Backpropagation is the name given to the process of training a neural network by updating its weights and biases.\n",
    "\n",
    "A neural networks learns to predict the correct values by continuously trying different values for the weights and then comparing the losses. If the loss function decreases, then the current weight is better than the previous, or vice versa. This means that the neural net has to go through many training (forward propagation) and update (backpropagation) cycles in order to get the optimal weights and biases. This cycle is generally referred to as the training phase, and the process of searching for the right weights is called optimisation.\n",
    "\n",
    "To correctly adjust the weights with respect to the loss the neural network calculates, you make use of calculus.\n",
    "\n",
    "### Using Calculus in Backpropagation\n",
    "After computing the output and loss in the forward propagation layer, you'll move to the backpropagation phase, where you calculate the derivatives backward, from the loss all the way up to the first weight and bias. To perform backpropagation in your neural network, you'll follow the steps listed below:\n",
    "\n",
    "Starting from the last layer, calculate the derivative of the loss with respect to the ouput $\\hat{y}$ as:\n",
    "\n",
    "<font size=\"6\"><center>$\\frac{dl}{d\\hat{y}}=\\frac{-y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}$</center></font>\n",
    "\n",
    "But how did you get the loss? Well, you calculated $\\sigma(Z_2)$. Now, what is the derivative of the loss with respect to $\\sigma(Z_2)$?\n",
    "\n",
    "$\\sigma(Z_2)$ is a combination of two functions, so you have to calculate two derivatives:\n",
    "\n",
    "First, calculate the derivative of sigmoid activation with respect to (w.r.t) the loss:\n",
    "<br><br>\n",
    "<font size=\"5\">\n",
    "    <center>\n",
    "        $\n",
    "        \\frac{d\\sigma}{dl}=\\sigma(x)\\times (1.0-\\sigma(x))\n",
    "        $\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "Then, you calculate the derivative of the loss w.r.t $Z_2$:\n",
    "<br><br>\n",
    "<font size=\"5\">\n",
    "    <center>\n",
    "        $\n",
    "        \\frac{dl}{dZ_2}=\\frac{dl}{d\\hat{y}}\\times\\frac{d\\sigma}{dl}\n",
    "        $\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "Now, how did you get Z2? You calculated a dot product between $A_1$ and $W_2$, and added a bias $b_2$. This means that you have to calculate the loss with respect to all these variables.\n",
    "<br><br>\n",
    "<font size=\"5\">\n",
    "    <center>\n",
    "        $\n",
    "        \\frac{dl}{dA_1}=W_2\\times\\frac{dl}{dZ_2}\n",
    "        $ <br><br>\n",
    "        $\n",
    "        \\frac{dl}{dW_2}=A_1\\times\\frac{dl}{dA_1}\n",
    "        $ <br><br>\n",
    "        $\n",
    "        \\frac{dl}{db_2}=\\frac{dl}{dZ_2}\n",
    "        $\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "And how did you get to $A_1$? You performed $ReLU(Z_1)$. So you take the derivative of $ReLU$ and $Z_1$ w.r.t the loss as well. The derivative of ReLU is 1 if the input is greater than 1, and 0 otherwise.\n",
    "\n",
    "You'll create a function to compute this and call it `dRelu`:\n",
    "<br><br>\n",
    "<font size=\"5\">\n",
    "    <center>\n",
    "        $\n",
    "        \\frac{dl}{dRelu}=dRelu\n",
    "        $ <br>\n",
    "        $\n",
    "        \\frac{dl}{dZ_1}=\\frac{dl}{dA_1}\\times dRelu\n",
    "        $\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "Next, how did you get $Z_1$? You computed the dot product between $X$ and $W_1$ and added the bias $b_1$. So you compute the derivate of all these variables, except the input $X$.\n",
    "<br><br>\n",
    "<font size=\"5\">\n",
    "    <center>\n",
    "        $\n",
    "        \\frac{dl}{dW_1}=X\\times\\frac{dl}{dZ_1}\n",
    "        $ <br>\n",
    "        $\n",
    "        \\frac{dl}{db_1}=\\frac{dl}{dZ_1}\n",
    "        $\n",
    "    </center>\n",
    "</font>\n",
    "\n",
    "For a detailed overview of hose these derivatives are calculated from scratch, [this post](https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60) is good.\n",
    "\n",
    "Let's write the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet5(NeuralNet4):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet4.__init__(self, *args)\n",
    "        \n",
    "    def dRelu(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def back_propagation(self, y_hat):\n",
    "        '''\n",
    "        Computes the derivatives and update the weights and bias \n",
    "        accordingly.\n",
    "        '''\n",
    "        y_inv = 1 - self.y\n",
    "        y_hat_inv = 1 - y_hat\n",
    "        \n",
    "        dl_wrt_yhat = np.divide(y_inv, self.eta(y_hat_inv)) - np.divide(self.y, self.eta(y_hat))\n",
    "        dl_wrt_sig = y_hat * (y_hat_inv)\n",
    "        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n",
    "        \n",
    "        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0, keepdims=True)\n",
    "        \n",
    "        dl_wrt_z1 = dl_wrt_A1 * self.dRelu(self.params['Z1'])\n",
    "        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0, keepdims=True)\n",
    "        \n",
    "        # update the weights and biases\n",
    "        self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n",
    "        self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n",
    "        self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n",
    "        self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation and Training of the Neural Network\n",
    "In the above cell, we used calculus to compute the derivatives of the weights and biases with respect to the loss. The model now knows how to change them. In the training phase, the neural network must perform the following:\n",
    "- Forward propagation\n",
    "- Backpropagation\n",
    "- Weight updates and calculated gradients\n",
    "- Repeat  \n",
    "\n",
    "What we're doing at the end of the cell is subtracting the derivative multiplied by a small value called the *learning rate* - a value which tells the network how big the update should be (our default value is 0.001).\n",
    "\n",
    "Now that we've added code to update the weights, let's make a new function called `fit()` which takes the input (X) and labels (Y) and calls the forward and backpropagation repeatedly for a specified number of iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet6(NeuralNet5):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet5.__init__(self, *args)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the neural network using the specified data and labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights()  # initalise weights and biases\n",
    "        \n",
    "        current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        print(f'Start time: {current_time}')\n",
    "        for i in range(self.iterations):\n",
    "            # print progress\n",
    "            portion_done = i / self.iterations\n",
    "            bars = round(portion_done*20)\n",
    "            print(f'[{\"=\"*bars}{\" \"*(20-bars)}]\\t{round(portion_done*100)}%', end='\\r')\n",
    "            \n",
    "            # fit\n",
    "            y_hat, loss = self.forward_propagation()\n",
    "            self.back_propagation(y_hat)\n",
    "            self.loss.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` method takes 2 parameters: X (input dataset) and y (labels). First, it saves the train and target to the class variable and then initialises the weights and biases by calling the `init_weights()` function. Then it loops through the specified number of iterations, performs forward and backpropagation, and saves the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "To make predictions, you simply make a forward pass on the test data, i.e. you use the saved weights and biases from the training phase. To make the process easier, let's add a function to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet7(NeuralNet6):\n",
    "    def __init__(self, *args):\n",
    "        NeuralNet6.__init__(self, *args)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts on a test datapoint\n",
    "        '''\n",
    "        Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        pred = self.sigmoid(Z2)\n",
    "        return np.round(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function passes the forward propagation and returns the final value (i.e. the prediction). The predictions are probability values between 0 and 1 (because of the sigmoid function). In order to interpret these probabilities, you can either round up the values or use a threshold function. To keep things simple, we just rounded up the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It Together\n",
    "Here's all the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    '''\n",
    "    A two layer neural network\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, layers=[13,8,1], learning_rate=0.001, iterations=100):\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "    \n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Initialise the weights from a random normal distribution\n",
    "        '''\n",
    "        np.random.seed(1) # seed the random number generator\n",
    "        self.params['W1'] = np.random.randn(self.layers[0], self.layers[1])\n",
    "        self.params['b1'] = np.random.randn(self.layers[1],)\n",
    "        self.params['W2'] = np.random.randn(self.layers[1], self.layers[2])\n",
    "        self.params['b2'] = np.random.randn(self.layers[2],)\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        '''\n",
    "        The ReLU activation function is to perform a threshold\n",
    "        operation to each input element where negative values\n",
    "        are set to zero.'''\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        '''\n",
    "        The sigmoid function takes in real numbers in any range and\n",
    "        squashes it to a real-valued output between 0 and 1.\n",
    "        '''\n",
    "        return 1/(1+np.exp(-Z))\n",
    "    \n",
    "    def eta(self, x):\n",
    "        ETA = 0.0000000001\n",
    "        return np.maximum(x, ETA)\n",
    "    \n",
    "    def entropy_loss(self, y, y_hat):\n",
    "        nsample = len(y)\n",
    "        y_hat_inv = 1.0 - y_hat\n",
    "        y_inv = 1.0 - y\n",
    "        y_hat = self.eta(y_hat) # clips value to avoid NaNs in log\n",
    "        y_hat_inv = self.eta(y_hat_inv)\n",
    "        \n",
    "        loss = -1/nsample * (\n",
    "            np.sum(np.multiply(np.log(y_hat), y) + \n",
    "                   np.multiply((y_inv), np.log(y_hat_inv))\n",
    "                  )\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def forward_propagation(self):\n",
    "        '''\n",
    "        Performs the forward propagation\n",
    "        '''\n",
    "        \n",
    "        Z1 = self.X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        y_hat = self.sigmoid(Z2)\n",
    "        loss = self.entropy_loss(self.y, y_hat)\n",
    "        \n",
    "        # save calculated parameters\n",
    "        self.params['Z1'] = Z1\n",
    "        self.params['Z2'] = Z2\n",
    "        self.params['A1'] = A1\n",
    "        \n",
    "        return y_hat, loss\n",
    "    \n",
    "    def dRelu(self, x):\n",
    "        x[x<=0] = 0\n",
    "        x[x>0] = 1\n",
    "        return x\n",
    "    \n",
    "    def back_propagation(self, y_hat):\n",
    "        '''\n",
    "        Computes the derivatives and update the weights and bias \n",
    "        accordingly.\n",
    "        '''\n",
    "        y_inv = 1 - self.y\n",
    "        y_hat_inv = 1 - y_hat\n",
    "        \n",
    "        dl_wrt_yhat = np.divide(y_inv, self.eta(y_hat_inv)) - np.divide(self.y, self.eta(y_hat))\n",
    "        dl_wrt_sig = y_hat * (y_hat_inv)\n",
    "        dl_wrt_z2 = dl_wrt_yhat * dl_wrt_sig\n",
    "        \n",
    "        dl_wrt_A1 = dl_wrt_z2.dot(self.params['W2'].T)\n",
    "        dl_wrt_w2 = self.params['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2 = np.sum(dl_wrt_z2, axis=0, keepdims=True)\n",
    "        \n",
    "        dl_wrt_z1 = dl_wrt_A1 * self.dRelu(self.params['Z1'])\n",
    "        dl_wrt_w1 = self.X.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1 = np.sum(dl_wrt_z1, axis=0, keepdims=True)\n",
    "        \n",
    "        # update the weights and biases\n",
    "        self.params['W1'] = self.params['W1'] - self.learning_rate * dl_wrt_w1\n",
    "        self.params['W2'] = self.params['W2'] - self.learning_rate * dl_wrt_w2\n",
    "        self.params['b1'] = self.params['b1'] - self.learning_rate * dl_wrt_b1\n",
    "        self.params['b2'] = self.params['b2'] - self.learning_rate * dl_wrt_b2\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the neural network using the specified data and labels\n",
    "        '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights()  # initalise weights and biases\n",
    "        \n",
    "        current_time = time.strftime(\"%H:%M:%S\", time.localtime())\n",
    "        print(f'Start time: {current_time}')\n",
    "        for i in range(self.iterations):\n",
    "            # print progress\n",
    "            portion_done = i / self.iterations\n",
    "            bars = round(portion_done*20)\n",
    "            print(f'[{\"=\"*bars}{\" \"*(20-bars)}]\\t{round(portion_done*100)}%', end='\\r')\n",
    "            \n",
    "            # fit\n",
    "            y_hat, loss = self.forward_propagation()\n",
    "            self.back_propagation(y_hat)\n",
    "            self.loss.append(loss)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predicts on a test datapoint\n",
    "        '''\n",
    "        Z1 = X.dot(self.params['W1']) + self.params['b1']\n",
    "        A1 = self.relu(Z1)\n",
    "        Z2 = A1.dot(self.params['W2']) + self.params['b2']\n",
    "        pred = self.sigmoid(Z2)\n",
    "        return np.round(pred)\n",
    "    \n",
    "    def acc(self, y, y_hat):\n",
    "        '''\n",
    "        Calculates the accuracy between the predicted values and the\n",
    "        truth labels\n",
    "        '''\n",
    "        acc = int(sum(y == y_hat) / len(y) * 100)\n",
    "        return acc\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        '''\n",
    "        Plots the loss curve\n",
    "        '''\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('logloss')\n",
    "        plt.title('Loss curve for training')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Neural Network\n",
    "We've already preprocessed our heart disease data and split it into train and test sets so we're ready to train and test the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 17:26:11\n",
      "[                    ]\t0%\r",
      "[                    ]\t1%\r",
      "[                    ]\t2%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t7%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t12%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t17%\r",
      "[====                ]\t18%\r",
      "[====                ]\t19%\r",
      "[====                ]\t20%\r",
      "[====                ]\t21%\r",
      "[====                ]\t22%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t27%\r",
      "[======              ]\t28%\r",
      "[======              ]\t29%\r",
      "[======              ]\t30%\r",
      "[======              ]\t31%\r",
      "[======              ]\t32%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t37%\r",
      "[========            ]\t38%\r",
      "[========            ]\t39%\r",
      "[========            ]\t40%\r",
      "[========            ]\t41%\r",
      "[========            ]\t42%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t47%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t52%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t57%\r",
      "[============        ]\t58%\r",
      "[============        ]\t59%\r",
      "[============        ]\t60%\r",
      "[============        ]\t61%\r",
      "[============        ]\t62%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t67%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t72%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t77%\r",
      "[================    ]\t78%\r",
      "[================    ]\t79%\r",
      "[================    ]\t80%\r",
      "[================    ]\t81%\r",
      "[================    ]\t82%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t87%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t92%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t97%\r",
      "[====================]\t98%\r",
      "[====================]\t99%\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[13,8,1], learning_rate=0.001, iterations=100)\n",
    "nn.fit(Xtrain, ytrain)  # train the model\n",
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 87%\n",
      "Test accuracy is 75%\n"
     ]
    }
   ],
   "source": [
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)\n",
    "\n",
    "print(f'Train accuracy is {nn.acc(ytrain, train_pred)}%')\n",
    "print(f'Test accuracy is {nn.acc(ytest, test_pred)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 17:26:28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                    ]\t0%\r",
      "[                    ]\t0%\r",
      "[                    ]\t0%\r",
      "[                    ]\t1%\r",
      "[                    ]\t1%\r",
      "[                    ]\t1%\r",
      "[                    ]\t1%\r",
      "[                    ]\t1%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t7%\r",
      "[=                   ]\t7%\r",
      "[=                   ]\t7%\r",
      "[=                   ]\t7%\r",
      "[=                   ]\t7%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t17%\r",
      "[===                 ]\t17%\r",
      "[===                 ]\t17%\r",
      "[===                 ]\t17%\r",
      "[===                 ]\t17%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t19%\r",
      "[====                ]\t19%\r",
      "[====                ]\t19%\r",
      "[====                ]\t19%\r",
      "[====                ]\t19%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t21%\r",
      "[====                ]\t21%\r",
      "[====                ]\t21%\r",
      "[====                ]\t21%\r",
      "[====                ]\t21%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t27%\r",
      "[=====               ]\t27%\r",
      "[=====               ]\t27%\r",
      "[=====               ]\t27%\r",
      "[=====               ]\t27%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t29%\r",
      "[======              ]\t29%\r",
      "[======              ]\t29%\r",
      "[======              ]\t29%\r",
      "[======              ]\t29%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t31%\r",
      "[======              ]\t31%\r",
      "[======              ]\t31%\r",
      "[======              ]\t31%\r",
      "[======              ]\t31%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t37%\r",
      "[=======             ]\t37%\r",
      "[=======             ]\t37%\r",
      "[=======             ]\t37%\r",
      "[=======             ]\t37%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t39%\r",
      "[========            ]\t39%\r",
      "[========            ]\t39%\r",
      "[========            ]\t39%\r",
      "[========            ]\t39%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t41%\r",
      "[========            ]\t41%\r",
      "[========            ]\t41%\r",
      "[========            ]\t41%\r",
      "[========            ]\t41%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t47%\r",
      "[=========           ]\t47%\r",
      "[=========           ]\t47%\r",
      "[=========           ]\t47%\r",
      "[=========           ]\t47%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t57%\r",
      "[===========         ]\t57%\r",
      "[===========         ]\t57%\r",
      "[===========         ]\t57%\r",
      "[===========         ]\t57%\r",
      "[============        ]\t58%\r",
      "[============        ]\t58%\r",
      "[============        ]\t58%\r",
      "[============        ]\t58%\r",
      "[============        ]\t58%\r",
      "[============        ]\t59%\r",
      "[============        ]\t59%\r",
      "[============        ]\t59%\r",
      "[============        ]\t59%\r",
      "[============        ]\t59%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t61%\r",
      "[============        ]\t61%\r",
      "[============        ]\t61%\r",
      "[============        ]\t61%\r",
      "[============        ]\t61%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t67%\r",
      "[=============       ]\t67%\r",
      "[=============       ]\t67%\r",
      "[=============       ]\t67%\r",
      "[=============       ]\t67%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t77%\r",
      "[===============     ]\t77%\r",
      "[===============     ]\t77%\r",
      "[===============     ]\t77%\r",
      "[===============     ]\t77%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t79%\r",
      "[================    ]\t79%\r",
      "[================    ]\t79%\r",
      "[================    ]\t79%\r",
      "[================    ]\t79%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t81%\r",
      "[================    ]\t81%\r",
      "[================    ]\t81%\r",
      "[================    ]\t81%\r",
      "[================    ]\t81%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t87%\r",
      "[=================   ]\t87%\r",
      "[=================   ]\t87%\r",
      "[=================   ]\t87%\r",
      "[=================   ]\t87%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t97%\r",
      "[=================== ]\t97%\r",
      "[=================== ]\t97%\r",
      "[=================== ]\t97%\r",
      "[=================== ]\t97%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t99%\r",
      "[====================]\t99%\r",
      "[====================]\t99%\r",
      "[====================]\t99%\r",
      "[====================]\t99%\r",
      "[====================]\t100%\r",
      "[====================]\t100%\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 100%\n",
      "Test accuracy is 70%\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[13,10,1], learning_rate=0.01, iterations=500)\n",
    "nn.fit(Xtrain, ytrain)\n",
    "nn.plot_loss()\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)\n",
    "\n",
    "print(f'Train accuracy is {nn.acc(ytrain, train_pred)}%')\n",
    "print(f'Test accuracy is {nn.acc(ytest, test_pred)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, although the plot shows a lower loss in the second model, the test accuracy in the second plot is actually worse than the first. This implies the second model is overtrained.\n",
    "\n",
    "BUT... these are *really* good results for a Neural Network built from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play area\n",
    "Feel free to make new models here and play around with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 17:30:01\n",
      "[                    ]\t0%\r",
      "[                    ]\t0%\r",
      "[                    ]\t1%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[                    ]\t2%\r",
      "[=                   ]\t3%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t4%\r",
      "[=                   ]\t5%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t6%\r",
      "[=                   ]\t7%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t8%\r",
      "[==                  ]\t9%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t10%\r",
      "[==                  ]\t11%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[==                  ]\t12%\r",
      "[===                 ]\t13%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t14%\r",
      "[===                 ]\t15%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t16%\r",
      "[===                 ]\t17%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t18%\r",
      "[====                ]\t19%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t20%\r",
      "[====                ]\t21%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[====                ]\t22%\r",
      "[=====               ]\t23%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t24%\r",
      "[=====               ]\t25%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t26%\r",
      "[=====               ]\t27%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t28%\r",
      "[======              ]\t29%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t30%\r",
      "[======              ]\t31%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[======              ]\t32%\r",
      "[=======             ]\t33%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t34%\r",
      "[=======             ]\t35%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t36%\r",
      "[=======             ]\t37%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t38%\r",
      "[========            ]\t39%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t40%\r",
      "[========            ]\t41%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[========            ]\t42%\r",
      "[=========           ]\t43%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t44%\r",
      "[=========           ]\t45%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t46%\r",
      "[=========           ]\t47%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t48%\r",
      "[==========          ]\t49%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t50%\r",
      "[==========          ]\t51%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[==========          ]\t52%\r",
      "[===========         ]\t53%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t54%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t55%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t56%\r",
      "[===========         ]\t57%\r",
      "[============        ]\t57%\r",
      "[============        ]\t58%\r",
      "[============        ]\t58%\r",
      "[============        ]\t59%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t60%\r",
      "[============        ]\t61%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[============        ]\t62%\r",
      "[=============       ]\t63%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t64%\r",
      "[=============       ]\t65%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t66%\r",
      "[=============       ]\t67%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t68%\r",
      "[==============      ]\t69%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t70%\r",
      "[==============      ]\t71%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[==============      ]\t72%\r",
      "[===============     ]\t73%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t74%\r",
      "[===============     ]\t75%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t76%\r",
      "[===============     ]\t77%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t78%\r",
      "[================    ]\t79%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t80%\r",
      "[================    ]\t81%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[================    ]\t82%\r",
      "[=================   ]\t83%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t84%\r",
      "[=================   ]\t85%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t86%\r",
      "[=================   ]\t87%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t88%\r",
      "[==================  ]\t89%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t90%\r",
      "[==================  ]\t91%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[==================  ]\t92%\r",
      "[=================== ]\t93%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t94%\r",
      "[=================== ]\t95%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t96%\r",
      "[=================== ]\t97%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t98%\r",
      "[====================]\t99%\r",
      "[====================]\t100%\r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is 94%\n",
      "Test accuracy is 74%\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNet(layers=[13, 20, 1], learning_rate=0.001, iterations=200)\n",
    "nn.fit(Xtrain, ytrain)\n",
    "nn.plot_loss()\n",
    "\n",
    "train_pred = nn.predict(Xtrain)\n",
    "test_pred = nn.predict(Xtest)\n",
    "\n",
    "print(f'Train accuracy is {nn.acc(ytrain, train_pred)}%')\n",
    "print(f'Test accuracy is {nn.acc(ytest, test_pred)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
